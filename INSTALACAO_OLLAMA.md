# ðŸ¤– InstalaÃ§Ã£o do Ollama para IA Local

## ðŸ“‹ **OpÃ§Ãµes DisponÃ­veis**

### **OpÃ§Ã£o 1: IA Local com Ollama (Recomendado para desenvolvimento)**
### **OpÃ§Ã£o 2: Fallback sem IA (JÃ¡ implementado)**
### **OpÃ§Ã£o 3: API Externa (OpenAI, Anthropic, etc.)**

---

## ðŸ–¥ï¸ **InstalaÃ§Ã£o no Windows**

### **MÃ©todo 1: Instalador Oficial**
1. Acesse: https://ollama.ai/download
2. Baixe o instalador para Windows
3. Execute como administrador
4. Ollama serÃ¡ instalado como serviÃ§o

### **MÃ©todo 2: Via PowerShell**
```powershell
# Baixar e instalar
iwr -useb https://ollama.ai/install.ps1 | iex

# Ou via Chocolatey
choco install ollama
```

### **ConfiguraÃ§Ã£o Windows**
```powershell
# Iniciar serviÃ§o
ollama serve

# Baixar modelo recomendado
ollama pull llama3.2:3b

# Testar
ollama run llama3.2:3b "OlÃ¡, como vocÃª estÃ¡?"
```

---

## ðŸ§ **InstalaÃ§Ã£o no VPS Linux**

### **Ubuntu/Debian**
```bash
# InstalaÃ§Ã£o automÃ¡tica
curl -fsSL https://ollama.ai/install.sh | sh

# Ou manual
wget https://ollama.ai/download/ollama-linux-amd64
sudo mv ollama-linux-amd64 /usr/local/bin/ollama
sudo chmod +x /usr/local/bin/ollama
```

### **CentOS/RHEL/Rocky Linux**
```bash
# InstalaÃ§Ã£o
curl -fsSL https://ollama.ai/install.sh | sh

# Ou via RPM
wget https://github.com/ollama/ollama/releases/download/v0.1.17/ollama-0.1.17-1.x86_64.rpm
sudo rpm -i ollama-0.1.17-1.x86_64.rpm
```

### **Configurar como ServiÃ§o (VPS)**
```bash
# Criar usuÃ¡rio para ollama
sudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama

# Criar arquivo de serviÃ§o
sudo tee /etc/systemd/system/ollama.service > /dev/null <<EOF
[Unit]
Description=Ollama Service
After=network-online.target

[Service]
ExecStart=/usr/local/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
Environment="OLLAMA_HOST=0.0.0.0"

[Install]
WantedBy=default.target
EOF

# Habilitar e iniciar
sudo systemctl daemon-reload
sudo systemctl enable ollama
sudo systemctl start ollama

# Verificar status
sudo systemctl status ollama
```

### **Baixar Modelos**
```bash
# Modelo leve (3B parÃ¢metros) - Recomendado
ollama pull llama3.2:3b

# Modelo mÃ©dio (7B parÃ¢metros)
ollama pull llama3.2:7b

# Modelo para cÃ³digo
ollama pull codellama:7b

# Listar modelos instalados
ollama list
```

---

## âš™ï¸ **ConfiguraÃ§Ã£o no Laravel**

### **Arquivo .env**
```env
# Ollama Configuration
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# Para VPS (se Ollama estiver em outro servidor)
OLLAMA_URL=http://seu-vps-ip:11434
```

### **ConfiguraÃ§Ã£o de Rede (VPS)**
```bash
# Permitir conexÃµes externas
sudo ufw allow 11434

# Ou configurar nginx proxy
sudo tee /etc/nginx/sites-available/ollama > /dev/null <<EOF
server {
    listen 80;
    server_name ollama.seudominio.com;
    
    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host \$host;
        proxy_set_header X-Real-IP \$remote_addr;
    }
}
EOF

sudo ln -s /etc/nginx/sites-available/ollama /etc/nginx/sites-enabled/
sudo nginx -t && sudo systemctl reload nginx
```

---

## ðŸ”„ **Alternativas sem IA Local**

### **1. Sistema Atual (Fallback)**
âœ… **JÃ¡ implementado** - Gera descriÃ§Ãµes usando templates quando Ollama nÃ£o estÃ¡ disponÃ­vel

### **2. API Externa - OpenAI**
```php
// Em config/services.php (jÃ¡ existe)
'openai' => [
    'api_key' => env('OPENAI_API_KEY'),
    'model' => env('OPENAI_MODEL', 'gpt-3.5-turbo'),
],
```

```env
# No .env
OPENAI_API_KEY=sk-sua-chave-aqui
OPENAI_MODEL=gpt-3.5-turbo
```

### **3. API Gratuita - Hugging Face**
```env
HUGGINGFACE_API_KEY=hf_sua-chave-aqui
HUGGINGFACE_MODEL=microsoft/DialoGPT-medium
```

---

## ðŸ§ª **Testar InstalaÃ§Ã£o**

### **Teste Local**
```bash
# Verificar se estÃ¡ rodando
curl http://localhost:11434/api/tags

# Testar geraÃ§Ã£o
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:3b",
  "prompt": "Descreva um disco de vinil de rock dos anos 80",
  "stream": false
}'
```

### **Teste no Laravel**
```bash
# No terminal do projeto
php artisan tinker

# Executar
$service = app(\App\Services\AIDescriptionService::class);
$result = $service->generateDescription([
    'artists' => 'Pink Floyd',
    'title' => 'The Wall',
    'year' => '1979'
]);
dd($result);
```

---

## ðŸ“Š **Requisitos de Sistema**

### **MÃ­nimo**
- **RAM**: 4GB
- **CPU**: 2 cores
- **Disco**: 5GB livres
- **Modelo**: llama3.2:3b (2GB)

### **Recomendado**
- **RAM**: 8GB+
- **CPU**: 4+ cores
- **Disco**: 10GB+ livres
- **Modelo**: llama3.2:7b (4GB)

---

## ðŸš€ **Status Atual**

âœ… **Sistema funciona SEM Ollama** - Usa templates como fallback
âœ… **Pronto para Ollama** - Detecta automaticamente se estÃ¡ disponÃ­vel
âœ… **ConfiguraÃ§Ã£o flexÃ­vel** - Suporta local e remoto

**RecomendaÃ§Ã£o**: Teste primeiro sem Ollama, depois instale quando necessÃ¡rio.
